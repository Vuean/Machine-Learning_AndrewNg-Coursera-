# 一、Introduction

## 1.1 欢迎参加《机器学习》课程

第一个视频主要讲了什么是机器学习，机器学习能做些什么事情。

机器学习是目前信息技术中最激动人心的方向之一。在这门课中，你将学习到这门技术的前沿，并可以自己实现学习机器学习的算法。

现实生活中，或许每天躲在使用机器学习。比如，使用谷歌、必应搜索时，使用Facebook或苹果的图片分类程序对照片进行分类时等等。

在这门课中，还将学习到关于机器学习的前沿状况。因为事实上只了解算法、数学并不能解决所关心的实际的问题。所以，本课程还将会花大量的时间做练习，让自己能实现每个这些算法，从而了解内部机理。

## 1.2 什么是机器学习？

Arthur Samuel：定义机器学习为，在进行特定编程的情况下，使计算机**具有学习能力**的领域。

Tom Mitchell：计算机程序从**经验E**中学习，解决**任务T**，达到**性能度量值P**，当且仅当，有了经验**E**后，经过**P**评判，程序在处理T时的性能有所提升。

主要的学习算法可分为：**监督学习（Supervised Learning）**和**无监督学习（Unsupervised Learning）**。本课中，我们将花费最多的精力来讨论这两种学习算法。而另一个会花费大量时间的任务是了解应用学习算法的实用建议。

## 1.3 监督学习

监督学习：给学习算法一个由“正确答案”组成的数据集，然后运用学习算法，算出更多的正确答案。用术语来讲，这叫做**回归问题**（regression problem）。我们试着推测出一个**连续值**的结果。还有**分类问题**（classfication problem），试着推出**离散的**输出值。

在处理分类问题过程中，为了让算法可以利用大量的特征或线索来推测，那么如何来存储这些特征？可通过**支持向量机**，能让计算机处理无限多个特征。

监督学习的基本思想是，我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测。其中包括有回归问题，即通过回归来推出一个连续的输出；和分类问题，其目标是推出一组离散的结果。

## 1.4 无监督学习

无监督学习中的数据可能没有任何标签、或者是相同的标签。针对数据集，无监督学习能将数据分为不同的簇，这也称为**聚类算法**（clustering algorithm）。

无监督学习或聚集有着大量的应用。它用于组织大型计算机集群。我有些朋友在大数据中心工作，那里有大型的计算机集群，他们想解决什么样的机器易于协同地工作，如果你能够让那些机器协同工作，你就能让你的数据中心工作得更高效。第二种应用就是社交网络的分析。所以已知你朋友的信息，比如你经常发**email**的，或是你**Facebook**的朋友、**谷歌+**圈子的朋友，我们能否自动地给出朋友的分组呢？即每组里的人们彼此都熟识，认识组里的所有人？还有市场分割。许多公司有大型的数据库，存储消费者信息。所以，你能检索这些顾客数据集，自动地发现市场分类，并自动地把顾客划分到不同的细分市场中，你才能自动并更有效地销售或不同的细分市场一起进行销售。这也是无监督学习，因为我们拥有所有的顾客数据，但我们没有提前知道是什么的细分市场，以及分别有哪些我们数据集中的顾客。我们不知道谁是在一号细分市场，谁在二号市场，等等。那我们就必须让算法从数据中发现这一切。最后，无监督学习也可用于天文数据分析，这些聚类算法给出了令人惊讶、有趣、有用的理论，解释了星系是如何诞生的。这些都是聚类的例子，聚类只是无监督学习中的一种。

# 二、单变量线性回归(Linear Regression with One Variable)

## 2.1 模型描述(Model representation)

上面学习了第一个学习算法——线性回归算法。本小节将看到这个算法的概况，更重要的是将了解监督学习的完整流程。

让我们通过一个例子来开始：这个例子是预测住房价格的，我们要使用一个数据集，数据集包含俄勒冈州波特兰市的住房价格。在这里，我要根据不同房屋尺寸所售出的价格，画出我的数据集。比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。那么，你可以做的一件事就是构建一个模型，也许是条直线，从这个数据模型上来看，也许你可以告诉你的朋友，他能以大约220000(美元)左右的价格卖掉这个房子。这就是监督学习算法的一个例子。

![图01_房价预测问题（回归问题）](https://github.com/Vuean/Machine_Learning_AndrewNg_Coursera/blob/main/Week1/images/%E5%9B%BE01_%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%97%AE%E9%A2%98%EF%BC%88%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%EF%BC%89.png)

它被称作监督学习是因为对于每个数据来说，我们给出了“正确的答案”。即告诉我们：根据我们的数据来说，房子实际的价格是多少，而且，更具体来说，这是一个**回归问题(Regression)**。回归一词指的是，我们根据之前的数据预测出一个准确的输出值。对于这个例子就是价格。

同时，还有另一种最常见的监督学习方式，叫做**分类问题(Classfication)**。当我们想要预测离散的输出值，例如，我们正在寻找癌症肿瘤，并想要确定肿瘤是良性的还是恶性的，这是只有0和1的离散输出。

更进一步来说，在监督学习中我们有一个数据集，这个数据集被称**训练集(training set)**，如下表所示：

![图02_训练集数据](https://github.com/Vuean/Machine_Learning_AndrewNg_Coursera/blob/main/Week1/images/%E5%9B%BE02_%E8%AE%AD%E7%BB%83%E9%9B%86%E6%95%B0%E6%8D%AE.png)

我们将要用来描述这个回归问题的标记如下:

- *m* 代表训练集中实例的数量

- *x* 代表输入变量/特征

- *y* 代表输出变量/目标变量

- (x, y) 代表训练集中的实例/训练样本

- (x<sup>(i)</sup>, y<sup>(i)</sup>) 代表第*i*个训练样本/实例

监督学习算法工作流程：

![图03_监督学习算法工作流程](https://github.com/Vuean/Machine_Learning_AndrewNg_Coursera/blob/main/Week1/images/%E5%9B%BE03_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png)

1. 向训练算法提供训练集，比如房价训练数据集。

2. 学习算法的功能则是输出一个函数，通常用*h*表示，称为**hypothesis**(**假设函数**)，在本例中该函数输入是是房屋尺寸大小*x*，输出为对应房子的价格*y*，*h*是一个从*x*到*y*的函数映射。

我将选择最初的使用规则*h*代表**hypothesis**，因而，要解决房价预测问题，我们实际上是要将训练集“喂”给我们的学习算法，进而学习得到一个假设*h*，然后将我们要预测的房屋的尺寸作为输入变量输入给*h*，预测出该房屋的交易价格作为输出变量输出为结果。那么，对于我们的房价预测问题，我们该如何表达*h*？

一种可能的表达方式为：h<sub>θ</sub>(x)=θ<sub>0</sub> + θ<sub>1</sub> * x，因为只含有一个特征/输入变量，因此这样的问题叫作**单变量线性回归问题**。

![图04_单变量线性回归](https://github.com/Vuean/Machine_Learning_AndrewNg_Coursera/blob/main/Week1/images/%E5%9B%BE04_%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.png)

## 2.2 代价函数(Cost function)

在这段视频中我们将定义代价函数的概念，这有助于我们弄清楚如何把最有可能的直线与我们的数据相拟合。

在线性回归中我们有一个像这样的训练集，$m$代表了训练样本的数量，比如 m = 47。而我们的假设函数，也就是用来进行预测的函数，是这样的线性函数形式：h<sub>θ</sub>(x)=θ<sub>0</sub> + θ<sub>1</sub> * x。

接下来我们会引入一些术语我们现在要做的便是为我们的模型选择合适的**参数**（**parameters**）θ<sub>0</sub> 和 θ<sub>1</sub>，在房价问题这个例子中便是直线的斜率和在*y* 轴上的截距。

我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是**建模误差**（**modeling error**）。

![图05_建模误差](https://github.com/Vuean/Machine_Learning_AndrewNg_Coursera/blob/main/Week1/images/%E5%9B%BE05_%E5%BB%BA%E6%A8%A1%E8%AF%AF%E5%B7%AE.png)

我们的目标便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数 $J \left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$最小。

我们绘制一个等高线图，三个坐标分别为θ<sub>0</sub> 和 θ<sub>1</sub> 和J(θ<sub>0</sub>, θ<sub>1</sub>)：

![图06_代价函数曲线](https://github.com/Vuean/Machine_Learning_AndrewNg_Coursera/blob/main/Week1/images/%E5%9B%BE06_%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E6%9B%B2%E7%BA%BF.png)

则可以看出在三维空间中存在一个使得J(θ<sub>0</sub>, θ<sub>1</sub>)最小的点。

代价函数也被称作**平方误差函数**，有时也被称为**平方误差代价函数**。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。

在后续课程中，我们还会谈论其他的代价函数，但我们刚刚讲的选择是对于大多数线性回归问题非常合理的。

也许这个函数J(θ<sub>0</sub>, θ<sub>1</sub>)有点抽象，可能你仍然不知道它的内涵，在接下来的几个视频里，我们要更进一步解释代价函数J的工作原理，并尝试更直观地解释它在计算什么，以及我们使用它的目的。

## 2.3 代价函数的直观理解I

在上一个视频中，我们给了代价函数一个数学上的定义。在这个视频里，让我们通过一些例子来获取一些直观的感受，看看代价函数到底是在干什么。

![图07_代价函数相关表达式](https://github.com/Vuean/Machine_Learning_AndrewNg_Coursera/blob/main/Week1/images/%E5%9B%BE07_%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E7%9B%B8%E5%85%B3%E8%A1%A8%E8%BE%BE%E5%BC%8F.png)

为了简单起见，可将上述代价函数进行简化：

![图08_简化后代价函数](https://github.com/Vuean/Machine_Learning_AndrewNg_Coursera/blob/main/Week1/images/%E5%9B%BE08_%E7%AE%80%E5%8C%96%E5%90%8E%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0.png)

简化后，假设函数h<sub>θ</sub>(x)是关于房子尺寸 *x* 的函数，代价函数J(θ<sub>1</sub>)则是关于参数θ<sub>1</sub>的函数。该函数如下图所示：

![图09_简化后函数图像](https://github.com/Vuean/Machine_Learning_AndrewNg_Coursera/blob/main/Week1/images/%E5%9B%BE09_%E7%AE%80%E5%8C%96%E5%90%8E%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F.png)

## 2.4 代价函数的直观理解II

这节课中，我们将更深入地学习代价函数的作用，这段视频的内容假设你已经认识等高线图，如果你对等高线图不太熟悉的话，这段视频中的某些内容你可能会听不懂，但不要紧，如果你跳过这段视频的话，也没什么关系，不听这节课对后续课程理解影响不大。

上一节中，将代价函数简化只考虑一个参数，可得到代价函数关于模型参数θ<sub>1</sub>的曲线是一条抛物线，现在考虑两个模型参数：θ<sub>0</sub>、 θ<sub>1</sub>，则函数绘制的结果将变得更为复杂，将会得到如下图所示的3D图形：

![图10_代价函数曲面图](https://github.com/Vuean/Machine_Learning_AndrewNg_Coursera/blob/main/Week1/images/%E5%9B%BE10_%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E6%9B%B2%E9%9D%A2%E5%9B%BE.png)

为了更直观的展示代价函数，将使用等高线图来展示3D曲面：

![图11_代价函数等高线图](https://github.com/Vuean/Machine_Learning_AndrewNg_Coursera/blob/main/Week1/images/%E5%9B%BE11_%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E7%AD%89%E9%AB%98%E7%BA%BF%E5%9B%BE.png)

图中每一个椭圆形，代表着一系列J(θ<sub>0</sub>, θ<sub>1</sub>)相等的点。

通过这些图形，我希望你能更好地理解这些代价函数 *J* 所表达的值是什么样的，它们对应的假设是什么样的，以及什么样的假设对应的点，更接近于代价函数 *J* 的最小值。

当然，我们真正需要的是一种有效的算法，能够自动地找出这些使代价函数 *J* 取最小值的参数θ<sub>0</sub>和θ<sub>1</sub>来。

我们也不希望编个程序把这些点画出来，然后人工的方法来读出这些点的数值，这很明显不是一个好办法。我们会遇到更复杂、更高维度、更多参数的情况，而这些情况是很难画出图的，因此更无法将其可视化，因此我们真正需要的是编写程序来找出这些最小化代价函数的θ<sub>0</sub>和θ<sub>1</sub>的值，在下一节视频中，我们将介绍一种算法，能够自动地找出能使代价函数*J*最小化的参数θ<sub>0</sub>和θ<sub>1</sub>的值。

## 2.5 梯度下降(Gradient descent)

梯度下降算法是很常见的算法，它不仅被用在线性回归上，同时还被广泛应用于机器学习的众多领域。

梯度下降是一个用来求函数最小值的算法，本节我们将使用梯度下降算法来求出代价函数J(θ<sub>0</sub>, θ<sub>1</sub>)的最小值。

梯度下降背后的思想是：开始时我们随机选择一个参数的组合(θ<sub>0</sub>, θ<sub>1</sub>, ..., θ<sub>n</sub>)，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到找到一个**局部最小值**（**local minimum**），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是**全局最小值**（**global minimum**），选择不同的初始参数组合，可能会找到不同的局部最小值。

![图12_梯度下降算法示意图]()

想象一下你正站立在山的这一点上，站立在你想象的公园这座红色山上，在梯度下降算法中，我们要做的就是旋转360度，看看我们的周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡上的这一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向迈着小碎步下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。

梯度下降背后数学原理：将反复执行下述公式，直到收敛。

![图13_梯度下降算法]()

其中，*α* 是学习率（**learning rate**），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。

![图14_梯度下降步骤]()

在梯度下降算法中，还有一个更微妙的问题，梯度下降中，我们要更新θ<sub>0</sub>和θ<sub>1</sub>，当 j=0 和 j=1时，会产生更新，所以你将更新J(θ<sub>0</sub>)和J(θ<sub>1</sub>)。实现梯度下降算法的微妙之处是，在这个表达式中，如果你要更新这个等式，你需要同时更新θ<sub>0</sub>和θ<sub>1</sub>，我的意思是在这个等式中，我们要这样更新：

θ<sub>0</sub> := θ<sub>0</sub> ，并更新θ<sub>1</sub> := θ<sub>1</sub>。

实现方法是：你应该计算公式右边的部分，通过那一部分计算出θ<sub>0</sub>和θ<sub>1</sub>的值，然后同时更新θ<sub>0</sub>和θ<sub>1</sub>。

让我进一步阐述这个过程：

在梯度下降算法中，这是正确实现同时更新的方法。我不打算解释为什么你需要同时更新，同时更新是梯度下降中的一种常用方法。我们之后会讲到，同步更新是更自然的实现方法。当人们谈到梯度下降时，他们的意思就是**同步更新**。

在接下来的视频中，我们要进入这个微分项的细节之中。我已经写了出来但没有真正定义，如果你已经修过微积分课程，如果你熟悉偏导数和导数，这其实就是这个微分项：

$\alpha \frac{\partial }{\partial {{\theta }_{0}}}J({{\theta }_{0}},{{\theta }_{1}})$，$\alpha \frac{\partial }{\partial {{\theta }_{1}}}J({{\theta }_{0}},{{\theta }_{1}})$。

如果你不熟悉微积分，不用担心，即使你之前没有看过微积分，或者没有接触过偏导数，在接下来的视频中，你会得到一切你需要知道，如何计算这个微分项的知识。

下一个视频中，希望我们能够给出实现梯度下降算法的所有知识 。

## 2.6 梯度下降的直观理解

在之前的视频中，我们给出了一个数学上关于梯度下降的定义，本次视频我们更深入研究一下，更直观地感受一下这个算法是做什么的，以及梯度下降算法的更新过程有什么意义。梯度下降算法如下：

${\theta_{j}}:={\theta_{j}}-\alpha \frac{\partial }{\partial {\theta_{j}}}J\left(\theta \right)$

描述：对 θ 赋值，使得 J(θ) 按梯度下降最快方向进行，一直迭代下去，最终得到局部最小值。其中 *α* 是学习率（**learning rate**），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。

![图15_梯度下降举例]()

对于这个问题，求导的目的，基本上可以说取这个红点的切线，就是这样一条红色的直线，刚好与函数相切于这一点，让我们看看这条红色直线的斜率，就是这条刚好与函数曲线相切的这条直线，这条直线的斜率正好是这个三角形的高度除以这个水平长度，现在，这条线有一个正斜率，也就是说它有正导数，因此，我得到的新的θ<sub>1</sub>，θ<sub>1</sub> 更新后等于θ<sub>1</sub> 减去一个正数乘以*α*。

这就是我梯度下降法的更新规则：${\theta_{j}}:={\theta_{j}}-\alpha \frac{\partial }{\partial {\theta_{j}}}J\left( \theta  \right)$

![图16_学习率规则]()

让我们来看看如果 *α* 太小或  *α* 太大会出现什么情况：

如果*α*太小了，即我的学习速率太小，结果就是只能这样像小宝宝一样一点点地挪动，去努力接近最低点，这样就需要很多步才能到达最低点，所以如果*α*太小的话，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。

如果*α*太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果*α*太大，它会导致无法收敛，甚至发散。

现在，我还有一个问题，当我第一次学习这个地方时，我花了很长一段时间才理解这个问题，如果我们预先把θ<sub>1</sub> 放在一个局部的最低点，你认为下一步梯度下降法会怎样工作？

![图17_局部最优时]()

假设你将 θ<sub>1</sub> 初始化在局部最低点，在这儿，它已经在一个局部的最优处或局部最低点。结果是局部最优点的导数将等于零，因为它是那条切线的斜率。这意味着你已经在局部最优点，它使得θ<sub>1</sub> 不再改变，也就是新的θ<sub>1</sub> 等于原来的θ<sub>1</sub>，因此，如果你的参数已经处于局部最低点，那么梯度下降法更新其实什么都没做，它不会改变参数的值。这也解释了为什么即使学习速率*α*保持不变时，梯度下降也可以收敛到局部最低点。

我们来看一个例子，这是代价函数J(θ)。

![图18_梯度下降]()

我想找到它的最小值，首先初始化我的梯度下降算法，在那个品红色的点初始化，如果我更新一步梯度下降，也许它会带我到这个点，因为这个点的导数是相当陡的。现在，在这个绿色的点，如果我再更新一步，你会发现我的导数，也即斜率，是没那么陡的。随着我接近最低点，我的导数越来越接近零。所以，梯度下降一步后，新的导数会变小一点点。然后我想再梯度下降一步，在这个绿点，我自然会用一个稍微跟刚才在那个品红点时比，再小一点的一步，到了新的红色点，更接近全局最低点了。因此这点的导数会比在绿点时更小，所以，我再进行一步梯度下降时，我的导数项是更小的，θ<sub>1</sub> 更新的幅度就会更小。所以随着梯度下降法的运行，你移动的幅度会自动变得越来越小，直到最终移动幅度非常小，你会发现，已经收敛到局部极小值。

回顾一下，在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，所以当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度，这就是梯度下降的做法。所以实际上没有必要再另外减小*α*。

这就是梯度下降算法，你可以用它来最小化任何代价函数*J*，不只是线性回归中的代价函数*J*。

在接下来的视频中，我们要用代价函数*J*，回到它的本质，线性回归中的代价函数。也就是我们前面得出的平方误差函数，结合梯度下降法，以及平方代价函数，我们会得出第一个机器学习算法，即线性回归算法。

## 2.7 梯度下降的线性回归

在以前的视频中我们谈到关于梯度下降算法，梯度下降是很常用的算法，它不仅被用在线性回归上和线性回归模型、平方误差代价函数。在这段视频中，我们要将梯度下降和代价函数结合。我们将用到此算法，并将其应用于具体的拟合直线的线性回归算法里。

梯度下降算法和线性回归算法比较如图：

![图19_梯度下降算法和线性回归算法]()

对我们之前的线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即：

![图20_代价函数的导数]()


$\frac{\partial }{\partial {{\theta }_{j}}}J({{\theta }_{0}},{{\theta }_{1}})=\frac{\partial }{\partial {{\theta }_{j}}}\frac{1}{2m}{{\sum\limits_{i=1}^{m}{\left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}}^{2}}$

$j=0$  时：$\frac{\partial }{\partial {{\theta }_{0}}}J({{\theta }_{0}},{{\theta }_{1}})=\frac{1}{m}{{\sum\limits_{i=1}^{m}{\left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}}}$

$j=1$  时：$\frac{\partial }{\partial {{\theta }_{1}}}J({{\theta }_{0}},{{\theta }_{1}})=\frac{1}{m}\sum\limits_{i=1}^{m}{\left( \left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)\cdot {{x}^{(i)}} \right)}$

则算法改写成：

![图21_梯度下降算法]()

**Repeat {**

​                ${\theta_{0}}:={\theta_{0}}-\alpha\frac{1}{m}\sum\limits_{i=1}^{m}{ \left({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}$

​                ${\theta_{1}}:={\theta_{1}}-\alpha\frac{1}{m}\sum\limits_{i=1}^{m}{\left( \left({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)\cdot {{x}^{(i)}} \right)}$

​               **}**

我们刚刚使用的算法，有时也称为批量梯度下降。实际上，在机器学习中，通常不太会给算法起名字，但这个名字”**批量梯度下降**”，指的是在梯度下降的每一步中，我们都**用到了所有的训练样本**，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有 *m* 个训练样本求和。因此，批量梯度下降法这个名字说明了我们需要考虑所有这一"批"训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种"批量"型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。在后面的课程中，我们也将介绍这些方法。

但就目前而言，应用刚刚学到的算法，你应该已经掌握了批量梯度算法，并且能把它应用到线性回归中了，这就是用于线性回归的梯度下降法。

如果你之前学过线性代数，有些同学之前可能已经学过高等线性代数，你应该知道有一种计算代价函数*J*最小值的数值解法，不需要梯度下降这种迭代算法。在后面的课程中，我们也会谈到这个方法，它可以在不需要多步梯度下降的情况下，也能解出代价函数*J*的最小值，这是另一种称为正规方程(**normal equations**)的方法。实际上在数据量较大的情况下，梯度下降法比正规方程要更适用一些。

现在我们已经掌握了梯度下降，我们可以在不同的环境中使用梯度下降法，我们还将在不同的机器学习问题中大量地使用它。所以，祝贺大家成功学会你的第一个机器学习算法。

在下一段视频中，告诉你泛化的梯度下降算法，这将使梯度下降更加强大。

# 线性代数回顾(Linear Algebra Review)

在接下来的一组视频中，我会对线性代数进行一个快速的复习回顾。如果你从来没有接触过向量和矩阵，那么这课件上所有的一切对你来说都是新知识，或者你之前对线性代数有所了解，但由于隔得久了，对其有所遗忘，那就请学习接下来的一组视频，我会快速地回顾你将用到的线性代数知识。

通过它们，你可以实现和使用更强大的线性回归模型。事实上，线性代数不仅仅在线性回归中应用广泛，它其中的矩阵和向量将有助于帮助我们实现之后更多的机器学习模型，并在计算上更有效率。正是因为这些矩阵和向量提供了一种有效的方式来组织大量的数据，特别是当我们处理巨大的训练集时，如果你不熟悉线性代数，如果你觉得线性代数看上去是一个复杂、可怕的概念，特别是对于之前从未接触过它的人，不必担心，事实上，为了实现机器学习算法，我们只需要一些非常非常基础的线性代数知识。通过接下来几个视频，你可以很快地学会所有你需要了解的线性代数知识。具体来说，为了帮助你判断是否有需要学习接下来的一组视频，我会讨论什么是矩阵和向量，谈谈如何加、减 、乘矩阵和向量，讨论逆矩阵和转置矩阵的概念。

如果你十分熟悉这些概念，那么你完全可以跳过这组关于线性代数的选修视频，但是如果你对这些概念仍有些许的不确定，不确定这些数字或这些矩阵的意思，那么请看一看下一组的视频，它会很快地教你一些你需要知道的线性代数的知识，便于之后编写机器学习算法和处理大量数据。

## 3.1 矩阵和向量

如图：这个是4×2矩阵，即4行2列，如$m$为行，$n$为列，那么$m×n$即4×2

![](../images/9fa04927c2bd15780f92a7fafb539179.png)

矩阵的维数即行数×列数

矩阵元素（矩阵项）：$A=\left[ \begin{matrix}   1402 & 191  \\   1371 & 821  \\   949 & 1437  \\   147 & 1448  \\\end{matrix} \right]$

$A_{ij}$指第$i$行，第$j$列的元素。

向量是一种特殊的矩阵，讲义中的向量一般都是列向量，如：
$y=\left[ \begin{matrix}   {460}  \\   {232}  \\   {315}  \\   {178}  \\\end{matrix} \right]$

为四维列向量（4×1）。

如下图为1索引向量和0索引向量，左图为1索引向量，右图为0索引向量，一般我们用1索引向量。

$y=\left[ \begin{matrix}   {{y}_{1}}  \\   {{y}_{2}}  \\   {{y}_{3}}  \\   {{y}_{4}}  \\\end{matrix} \right]$，$y=\left[ \begin{matrix}   {{y}_{0}}  \\   {{y}_{1}}  \\   {{y}_{2}}  \\   {{y}_{3}}  \\\end{matrix} \right]$

### 3.2 加法和标量乘法

参考视频: 3 - 2 - Addition and Scalar Multiplication (7 min).mkv

矩阵的加法：行列数相等的可以加。

例：

![](../images/ffddfddfdfd.png)

矩阵的乘法：每个元素都要乘

![](../images/fdddddd.png)

组合算法也类似。

### 3.3 矩阵向量乘法

参考视频: 3 - 3 - Matrix Vector Multiplication (14 min).mkv

矩阵和向量的乘法如图：$m×n$的矩阵乘以$n×1$的向量，得到的是$m×1$的向量

![](../images/437ae2333f00286141abe181a1b7c44a.png)

算法举例：

![](../images/b2069e4b3e12618f5405500d058118d7.png)

### 3.4 矩阵乘法

参考视频: 3 - 4 - Matrix Matrix Multiplication (11 min).mkv

矩阵乘法：

$m×n$矩阵乘以$n×o$矩阵，变成$m×o$矩阵。

如果这样说不好理解的话就举一个例子来说明一下，比如说现在有两个矩阵$A$和$B$，那么它们的乘积就可以表示为图中所示的形式。

![](../images/1a9f98df1560724713f6580de27a0bde.jpg)
![](../images/5ec35206e8ae22668d4b4a3c3ea7b292.jpg)

### 3.5 矩阵乘法的性质

参考视频: 3 - 5 - Matrix Multiplication Properties (9 min).mkv

矩阵乘法的性质：

矩阵的乘法不满足交换律：$A×B≠B×A$

矩阵的乘法满足结合律。即：$A×(B×C)=(A×B)×C$

单位矩阵：在矩阵的乘法中，有一种矩阵起着特殊的作用，如同数的乘法中的1,我们称这种矩阵为单位矩阵．它是个方阵，一般用 $I$ 或者 $E$ 表示，本讲义都用 $I$ 代表单位矩阵，从左上角到右下角的对角线（称为主对角线）上的元素均为1以外全都为0。如：

$A{{A}^{-1}}={{A}^{-1}}A=I$

对于单位矩阵，有$AI=IA=A$

### 3.6 逆、转置

参考视频: 3 - 6 - Inverse and Transpose (11 min).mkv

矩阵的逆：如矩阵$A$是一个$m×m$矩阵（方阵），如果有逆矩阵，则：$A{{A}^{-1}}={{A}^{-1}}A=I$

我们一般在**OCTAVE**或者**MATLAB**中进行计算矩阵的逆矩阵。

矩阵的转置：设$A$为$m×n$阶矩阵（即$m$行$n$列），第$i $行$j $列的元素是$a(i,j)$，即：$A=a(i,j)$

定义$A$的转置为这样一个$n×m$阶矩阵$B$，满足$B=a(j,i)$，即 $b (i,j)=a(j,i)$（$B$的第$i$行第$j$列元素是$A$的第$j$行第$i$列元素），记${{A}^{T}}=B$。(有些书记为A'=B）

直观来看，将$A$的所有元素绕着一条从第1行第1列元素出发的右下方45度的射线作镜面反转，即得到$A$的转置。

例：

${{\left| \begin{matrix}   a& b  \\   c& d  \\   e& f  \\\end{matrix} \right|}^{T}}=\left|\begin{matrix}   a& c & e  \\   b& d & f  \\\end{matrix} \right|$

矩阵的转置基本性质:

$ {{\left( A\pm B \right)}^{T}}={{A}^{T}}\pm {{B}^{T}} $
${{\left( A\times B \right)}^{T}}={{B}^{T}}\times {{A}^{T}}$
${{\left( {{A}^{T}} \right)}^{T}}=A $
${{\left( KA \right)}^{T}}=K{{A}^{T}} $

**matlab**中矩阵转置：直接打一撇，`x=y'`。
