# 一、Introduction

## 1.1 欢迎参加《机器学习》课程

第一个视频主要讲了什么是机器学习，机器学习能做些什么事情。

机器学习是目前信息技术中最激动人心的方向之一。在这门课中，你将学习到这门技术的前沿，并可以自己实现学习机器学习的算法。

现实生活中，或许每天躲在使用机器学习。比如，使用谷歌、必应搜索时，使用Facebook或苹果的图片分类程序对照片进行分类时等等。

在这门课中，还将学习到关于机器学习的前沿状况。因为事实上只了解算法、数学并不能解决所关心的实际的问题。所以，本课程还将会花大量的时间做练习，让自己能实现每个这些算法，从而了解内部机理。

## 1.2 什么是机器学习？

Arthur Samuel：定义机器学习为，在进行特定编程的情况下，使计算机**具有学习能力**的领域。

Tom Mitchell：计算机程序从**经验E**中学习，解决**任务T**，达到**性能度量值P**，当且仅当，有了经验**E**后，经过**P**评判，程序在处理T时的性能有所提升。

主要的学习算法可分为：**监督学习（Supervised Learning）**和**无监督学习（Unsupervised Learning）**。本课中，我们将花费最多的精力来讨论这两种学习算法。而另一个会花费大量时间的任务是了解应用学习算法的实用建议。

## 1.3 监督学习

监督学习：给学习算法一个由“正确答案”组成的数据集，然后运用学习算法，算出更多的正确答案。用术语来讲，这叫做**回归问题**（regression problem）。我们试着推测出一个**连续值**的结果。还有**分类问题**（classfication problem），试着推出**离散的**输出值。

在处理分类问题过程中，为了让算法可以利用大量的特征或线索来推测，那么如何来存储这些特征？可通过**支持向量机**，能让计算机处理无限多个特征。

监督学习的基本思想是，我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测。其中包括有回归问题，即通过回归来推出一个连续的输出；和分类问题，其目标是推出一组离散的结果。

## 1.4 无监督学习

无监督学习中的数据可能没有任何标签、或者是相同的标签。针对数据集，无监督学习能将数据分为不同的簇，这也称为**聚类算法**（clustering algorithm）。

无监督学习或聚集有着大量的应用。它用于组织大型计算机集群。我有些朋友在大数据中心工作，那里有大型的计算机集群，他们想解决什么样的机器易于协同地工作，如果你能够让那些机器协同工作，你就能让你的数据中心工作得更高效。第二种应用就是社交网络的分析。所以已知你朋友的信息，比如你经常发**email**的，或是你**Facebook**的朋友、**谷歌+**圈子的朋友，我们能否自动地给出朋友的分组呢？即每组里的人们彼此都熟识，认识组里的所有人？还有市场分割。许多公司有大型的数据库，存储消费者信息。所以，你能检索这些顾客数据集，自动地发现市场分类，并自动地把顾客划分到不同的细分市场中，你才能自动并更有效地销售或不同的细分市场一起进行销售。这也是无监督学习，因为我们拥有所有的顾客数据，但我们没有提前知道是什么的细分市场，以及分别有哪些我们数据集中的顾客。我们不知道谁是在一号细分市场，谁在二号市场，等等。那我们就必须让算法从数据中发现这一切。最后，无监督学习也可用于天文数据分析，这些聚类算法给出了令人惊讶、有趣、有用的理论，解释了星系是如何诞生的。这些都是聚类的例子，聚类只是无监督学习中的一种。

# 二、单变量线性回归(Linear Regression with One Variable)

## 2.1 模型描述(Model representation)

上面学习了第一个学习算法——线性回归算法。本小节将看到这个算法的概况，更重要的是将了解监督学习的完整流程。

让我们通过一个例子来开始：这个例子是预测住房价格的，我们要使用一个数据集，数据集包含俄勒冈州波特兰市的住房价格。在这里，我要根据不同房屋尺寸所售出的价格，画出我的数据集。比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。那么，你可以做的一件事就是构建一个模型，也许是条直线，从这个数据模型上来看，也许你可以告诉你的朋友，他能以大约220000(美元)左右的价格卖掉这个房子。这就是监督学习算法的一个例子。

![图01_房价预测问题（回归问题）](https://github.com/Vuean/Machine_Learning_AndrewNg_Coursera/blob/main/Week1/images/%E5%9B%BE01_%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%97%AE%E9%A2%98%EF%BC%88%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%EF%BC%89.png)

它被称作监督学习是因为对于每个数据来说，我们给出了“正确的答案”。即告诉我们：根据我们的数据来说，房子实际的价格是多少，而且，更具体来说，这是一个**回归问题(Regression)**。回归一词指的是，我们根据之前的数据预测出一个准确的输出值。对于这个例子就是价格。

同时，还有另一种最常见的监督学习方式，叫做**分类问题(Classfication)**。当我们想要预测离散的输出值，例如，我们正在寻找癌症肿瘤，并想要确定肿瘤是良性的还是恶性的，这是只有0和1的离散输出。

更进一步来说，在监督学习中我们有一个数据集，这个数据集被称**训练集(training set)**，如下表所示：

![图02_训练集数据](https://github.com/Vuean/Machine_Learning_AndrewNg_Coursera/blob/main/Week1/images/%E5%9B%BE02_%E8%AE%AD%E7%BB%83%E9%9B%86%E6%95%B0%E6%8D%AE.png)

我们将要用来描述这个回归问题的标记如下:

- *m* 代表训练集中实例的数量

- *x* 代表输入变量/特征

- *y* 代表输出变量/目标变量

- (x, y) 代表训练集中的实例/训练样本

- (x<sup>(i)</sup>, y<sup>(i)</sup>) 代表第*i*个训练样本/实例

监督学习算法工作流程：

![图03_监督学习算法工作流程](https://github.com/Vuean/Machine_Learning_AndrewNg_Coursera/blob/main/Week1/images/%E5%9B%BE03_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png)

1. 向训练算法提供训练集，比如房价训练数据集。

2. 学习算法的功能则是输出一个函数，通常用*h*表示，称为**hypothesis**(**假设函数**)，在本例中该函数输入是是房屋尺寸大小*x*，输出为对应房子的价格*y*，*h*是一个从*x*到*y*的函数映射。

我将选择最初的使用规则*h*代表**hypothesis**，因而，要解决房价预测问题，我们实际上是要将训练集“喂”给我们的学习算法，进而学习得到一个假设*h*，然后将我们要预测的房屋的尺寸作为输入变量输入给*h*，预测出该房屋的交易价格作为输出变量输出为结果。那么，对于我们的房价预测问题，我们该如何表达*h*？

一种可能的表达方式为：h<sub>θ</sub>(x)=θ<sub>0</sub> + θ<sub>1</sub> * x，因为只含有一个特征/输入变量，因此这样的问题叫作**单变量线性回归问题**。

![图04_单变量线性回归](https://github.com/Vuean/Machine_Learning_AndrewNg_Coursera/blob/main/Week1/images/%E5%9B%BE04_%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.png)

## 2.2 代价函数(Cost function)

在这段视频中我们将定义代价函数的概念，这有助于我们弄清楚如何把最有可能的直线与我们的数据相拟合。

在线性回归中我们有一个像这样的训练集，$m$代表了训练样本的数量，比如 m = 47。而我们的假设函数，也就是用来进行预测的函数，是这样的线性函数形式：h<sub>θ</sub>(x)=θ<sub>0</sub> + θ<sub>1</sub> * x。

接下来我们会引入一些术语我们现在要做的便是为我们的模型选择合适的**参数**（**parameters**）θ<sub>0</sub> 和 θ<sub>1</sub>，在房价问题这个例子中便是直线的斜率和在*y* 轴上的截距。

我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是**建模误差**（**modeling error**）。

![图05_建模误差]()

我们的目标便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数 $J \left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$最小。

我们绘制一个等高线图，三个坐标分别为θ<sub>0</sub> 和 θ<sub>1</sub> 和J(θ<sub>0</sub>, θ<sub>1</sub>)：

![图06_代价函数曲线]()

则可以看出在三维空间中存在一个使得J(θ<sub>0</sub>, θ<sub>1</sub>)最小的点。

代价函数也被称作**平方误差函数**，有时也被称为**平方误差代价函数**。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。

在后续课程中，我们还会谈论其他的代价函数，但我们刚刚讲的选择是对于大多数线性回归问题非常合理的。

也许这个函数J(θ<sub>0</sub>, θ<sub>1</sub>)有点抽象，可能你仍然不知道它的内涵，在接下来的几个视频里，我们要更进一步解释代价函数J的工作原理，并尝试更直观地解释它在计算什么，以及我们使用它的目的。

## 2.3 代价函数的直观理解I

在上一个视频中，我们给了代价函数一个数学上的定义。在这个视频里，让我们通过一些例子来获取一些直观的感受，看看代价函数到底是在干什么。

![图07_代价函数相关表达式]()

为了简单起见，可将上述代价函数进行简化：

![图08_简化后代价函数]()

简化后，假设函数h<sub>θ</sub>(x)是关于房子尺寸 *x* 的函数，代价函数J(θ<sub>1</sub>)则是关于参数θ<sub>1</sub>的函数。该函数如下图所示：

![图09_简化后函数图像]()

## 2.4 代价函数的直观理解II

这节课中，我们将更深入地学习代价函数的作用，这段视频的内容假设你已经认识等高线图，如果你对等高线图不太熟悉的话，这段视频中的某些内容你可能会听不懂，但不要紧，如果你跳过这段视频的话，也没什么关系，不听这节课对后续课程理解影响不大。

上一节中，将代价函数简化只考虑一个参数，可得到代价函数关于模型参数θ<sub>1</sub>的曲线是一条抛物线，现在考虑两个模型参数：θ<sub>0</sub>、 θ<sub>1</sub>，则函数绘制的结果将变得更为复杂，将会得到如下图所示的3D图形：

![图10_代价函数曲面图]()

为了更直观的展示代价函数，将使用等高线图来展示3D曲面：

![图11_代价函数等高线图]()

图中每一个椭圆形，代表着一系列J(θ<sub>0</sub>, θ<sub>1</sub>)相等的点。

## 2.5 梯度下降(Gradient descent)





# Linear Algebra Review

线性代数回顾

